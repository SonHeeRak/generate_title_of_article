{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집기간 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_range = []\n",
    "start = datetime.datetime.strptime('2018.01.01', '%Y.%m.%d')\n",
    "end = datetime.datetime.strptime('2018.12.31', '%Y.%m.%d')\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end - start).days)]\n",
    "\n",
    "for date in date_generated:\n",
    "    days_range.append(date.strftime('%Y.%m.%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018.01.01', '2018.01.02', '2018.01.03']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_range[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하루 전체 기사 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_total_article_num(html):\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    daily_total_article_num = int(soup.find('div', attrs={'class': 'title_desc all_my'}).text.split('/')[1].strip()[:-1].replace(',', ''))\n",
    "    return daily_total_article_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제목과 네이버뉴스 주소 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_url(url, start_num):\n",
    "    html = requests.get(url.format(query=query, start_date=day, end_date=day, start_date2=day2, end_date2=day2, start_num=start_num))\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    p = re.compile('sp_nws')\n",
    "    news_list = soup.find_all('li', attrs={'id': p})\n",
    "    \n",
    "    title_list = []\n",
    "    article_url_list = []\n",
    "    source_list = []\n",
    "    for idx, news in enumerate(news_list):\n",
    "        if news.find('dd', attrs={'class': 'txt_inline'}).find('a', attrs={'class': '_sp_each_url'}):  # 네이버뉴스 유무\n",
    "            title = news.find('dt').text\n",
    "            article_url = news.find('dd', attrs={'class': 'txt_inline'}).find('a', attrs={'class': '_sp_each_url'}).get('href')\n",
    "            source = news.find('dd', attrs={'class': 'txt_inline'}).find('span', attrs={'class': '_sp_each_source'}).text\n",
    "            title_list.append(title)\n",
    "            article_url_list.append(article_url)\n",
    "            source_list.append(source)\n",
    "            \n",
    "    return title_list, article_url_list, source_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본문 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(article_url_list):\n",
    "    content_list = []\n",
    "    for url in article_url_list:\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        if soup.find('div', attrs={'id': 'newsEndContents'}):\n",
    "            text = soup.find('div', attrs={'id': 'newsEndContents'}).text.strip()\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = ' '.join(text.split())\n",
    "        elif soup.find('div', attrs={'id': 'articleBodyContents'}):\n",
    "            text = soup.find('div', attrs={'id': 'articleBodyContents'}).text.replace('\\n', ' ')\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = ' '.join(text.split())\n",
    "            text = text.replace('// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}', '').strip()\n",
    "        elif soup.find('div', attrs={'id': 'articeBody'}):\n",
    "            text = soup.find('div', attrs={'id': 'articeBody'}).text.replace('\\n', ' ').strip()\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = ' '.join(text.split())\n",
    "        else:\n",
    "            print('또 다른 유형이 나왔습니다.')\n",
    "        content_list.append(text)\n",
    "        \n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018년 전체 KLPGA 기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a838f4c3cc24da7b2fe605a92ead24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=364.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'KLPGA'\n",
    "url = '''https://search.naver.com/search.naver?&where=news&query={query}&sm=tab_pge&sort=1&photo=0&field=0&reporter_article=&pd=3\n",
    "        &ds={start_date}&de={end_date}&docid=&nso=so:dd,p:from{start_date2}to{end_date2},a:all&mynews=0&start={start_num}&refresh_start=0'''\n",
    "title = []\n",
    "article_url = []\n",
    "source = []\n",
    "content = []\n",
    "\n",
    "for day in tqdm(days_range):\n",
    "    try:\n",
    "        start_num = 1\n",
    "        day2 = day.replace('.', '')\n",
    "        html = requests.get(url.format(query=query, start_date=day, end_date=day, start_date2=day2, end_date2=day2, start_num=start_num))\n",
    "        daily_total_article_num = get_daily_total_article_num(html)\n",
    "        for start_num in range(daily_total_article_num // 10 + 1):\n",
    "            title_list, article_url_list, source_list = get_title_url(url, start_num)\n",
    "            content_list = get_content(article_url_list)\n",
    "            title += title_list\n",
    "            article_url += article_url_list\n",
    "            source += source_list\n",
    "            content += content_list\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27016, 27016)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title), len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'url': url, 'title': title, 'source': source, 'content': content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://search.naver.com/search.naver?&amp;where=n...</td>\n",
       "      <td>한국 낭자들, 이번에는 LPGA 15승 넘어설까</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>2018시즌 역대 최다승 도전미국 골프전문 매체 골프채널이 선정한 지난해 미국여자프...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://search.naver.com/search.naver?&amp;where=n...</td>\n",
       "      <td>2018시즌 역대 최다승 합작 도전.. 코리안 군단의 위세는 계속된다</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>작년 10명 총 15승 합작유소연.박성현.김인경 등 최다승 합작 달성여부 관심 LP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://search.naver.com/search.naver?&amp;where=n...</td>\n",
       "      <td>LPGA 한국 자매들 당찬 ‘16승’ 출사표</td>\n",
       "      <td>서울신문</td>\n",
       "      <td>[서울신문]지난해 역대 최다승 타이기록인 15승(메이저대회 3승 포함)을 합작한 ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://search.naver.com/search.naver?&amp;where=n...</td>\n",
       "      <td>[데스크 칼럼] 새해, 실패에 대한 단상</td>\n",
       "      <td>부산일보</td>\n",
       "      <td>/김마선 교육팀장 새해가 또 밝았다. 무심한 시간에다 '마디'를 만들어 새 희망을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://search.naver.com/search.naver?&amp;where=n...</td>\n",
       "      <td>'황금개띠 골퍼' 전인지 \"새 후원사 KB금융 '천군만마'… 성장통 끝냈으니 훨...</td>\n",
       "      <td>한국경제</td>\n",
       "      <td>도전 2018! (5) 지난해 LPGA 준우승만 5번우승 못했지만 꾸준했던 한해허리...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://search.naver.com/search.naver?&where=n...   \n",
       "1  https://search.naver.com/search.naver?&where=n...   \n",
       "2  https://search.naver.com/search.naver?&where=n...   \n",
       "3  https://search.naver.com/search.naver?&where=n...   \n",
       "4  https://search.naver.com/search.naver?&where=n...   \n",
       "\n",
       "                                             title  source  \\\n",
       "0                       한국 낭자들, 이번에는 LPGA 15승 넘어설까    세계일보   \n",
       "1           2018시즌 역대 최다승 합작 도전.. 코리안 군단의 위세는 계속된다  파이낸셜뉴스   \n",
       "2                         LPGA 한국 자매들 당찬 ‘16승’ 출사표    서울신문   \n",
       "3                           [데스크 칼럼] 새해, 실패에 대한 단상    부산일보   \n",
       "4  '황금개띠 골퍼' 전인지 \"새 후원사 KB금융 '천군만마'… 성장통 끝냈으니 훨...    한국경제   \n",
       "\n",
       "                                             content  \n",
       "0  2018시즌 역대 최다승 도전미국 골프전문 매체 골프채널이 선정한 지난해 미국여자프...  \n",
       "1  작년 10명 총 15승 합작유소연.박성현.김인경 등 최다승 합작 달성여부 관심 LP...  \n",
       "2  [서울신문]지난해 역대 최다승 타이기록인 15승(메이저대회 3승 포함)을 합작한 ‘...  \n",
       "3  /김마선 교육팀장 새해가 또 밝았다. 무심한 시간에다 '마디'를 만들어 새 희망을 ...  \n",
       "4  도전 2018! (5) 지난해 LPGA 준우승만 5번우승 못했지만 꾸준했던 한해허리...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_2018 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018년 전체 KLPGA 네이버기사 csv파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_2018.to_csv('data/klpga_2018_article.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
